#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ä¸­åè‹±çƒˆç½‘ Â· æµå¼å…¨é‡çˆ¬è™«  v1.3.3
-----------------------------------------------------
â˜… æ–°å¢
    Â· column_explanations  â€”â€” ç»Ÿä¸€çš„ã€Œè‹± -> ä¸­æ–‡ã€å­—æ®µæ˜ å°„
    Â· ä¿å­˜å‰ .rename(...)  â€”â€” å†™ CSV æ—¶è‡ªåŠ¨ç”¨ä¸­æ–‡è¡¨å¤´
"""

from __future__ import annotations
import os, sys, json, time, random, hashlib, pickle, pathlib, argparse, logging
from typing import Dict, Any, List, Set

import requests
import pandas as pd
from tqdm.auto import tqdm

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 0. CLI
p = argparse.ArgumentParser(description="ä¸­åè‹±çƒˆç½‘çˆ¬è™« Â· Break-Point + Retry + ä¸­æ–‡è¡¨å¤´")
p.add_argument("--chunks",       type=int,   default=1,  help="æ¯å¤šå°‘é¡µå†™ 1 ä¸ª CSV")
p.add_argument("--pages",        type=int,   default=None, help="åªæŠ“å‰ N é¡µ (è°ƒè¯•)")
p.add_argument("--sleep-base",   type=float, default=0.1, help="æ¯é¡µå›ºå®šä¼‘çœ ç§’")
p.add_argument("--sleep-jitter", type=float, default=0.1, help="é™„åŠ æŠ–åŠ¨ (0~x) ç§’")
p.add_argument("--cool-every",   type=int,   default=200,   help="æ¯ N é¡µå†é¢å¤–ä¼‘çœ ")
p.add_argument("--cool-time",    type=float, default=0.1,  help="é¢å¤–ä¼‘çœ ç§’æ•°")
p.add_argument("--verbose", action="store_true")
args = p.parse_args()

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 1. å¸¸é‡
BASE          = r"E:\çˆ¬è™«å®æˆ˜\ä¸­åè‹±çƒˆç½‘"               # â† è‡ªè¡Œä¿®æ”¹
AREA_FILE     = os.path.join(BASE, "000000000000.json")
TMP_DIR       = os.path.join(BASE, "chunks")
ID_CACHE      = os.path.join(BASE, "seen_ids.pkl")
PAGE_FILE     = os.path.join(BASE, "current_page.txt")

UA            = ("Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
                 "AppleWebKit/537.36 (KHTML, like Gecko) Chrome/137.0.0.0 Safari/537.36")

PAGE_SIZE     = 10
TOTAL_PAGES   = 187_629
CHUNK_PAGES   = args.chunks

TIMEOUT       = 30000
MAX_RETRY     = 500000

pathlib.Path(TMP_DIR).mkdir(parents=True, exist_ok=True)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 2. æ—¥å¿—
level = logging.DEBUG if args.verbose else logging.INFO
logging.basicConfig(level=level,
                    format="%(asctime)s %(levelname)-8s %(message)s",
                    datefmt="%H:%M:%S",
                    handlers=[logging.StreamHandler(sys.stdout)])
log = logging.getLogger("martyrs")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 3. è¡Œæ”¿åŒºåˆ’ â†’ åœ°å€è§£æ
with open(AREA_FILE, "r", encoding="utf-8") as f:
    raw = json.load(f)

AREA: Dict[str, str] = {}
def _dfs(nodes):
    for n in nodes:
        AREA[n["orgId"]] = n["deptName"]
        _dfs(n.get("children", []))
_dfs(raw if isinstance(raw, list) else [raw])
log.info("è¡Œæ”¿åŒºåˆ’ %d æ¡", len(AREA))

def addr(code: str) -> str:
    if not code:
        return ""
    return "".join(AREA.get(seg, "") for seg in
                   (code[:2] + "000000000000",
                    code[:4] + "00000000",
                    code[:6] + "000000",
                    code))

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 4. å­—æ®µä¸­æ–‡æ˜ å°„
column_explanations: Dict[str, str] = {
    "ID": "å”¯ä¸€æ ‡è¯†ç¬¦",
    "status": "çŠ¶æ€", "createBy": "åˆ›å»ºè€…", "createTime": "åˆ›å»ºæ—¶é—´",
    "updateBy": "æ›´æ–°è€…", "updateTime": "æ›´æ–°æ—¶é—´", "updateDate": "æ›´æ–°æ—¥æœŸ",
    "remark": "å¤‡æ³¨", "delFlag": "åˆ é™¤æ ‡è®°", "searchValue": "æœç´¢å€¼",
    "dataScope": "æ•°æ®èŒƒå›´",

    "mmdrGuid": "çƒˆå£«ID",
    "mmdrName": "å§“å", "mmdrAsname": "åˆ«å",
    "mmdrSexId": "æ€§åˆ«ID", "mmdrSex": "æ€§åˆ«",

    "mmdrShengId": "çœID", "mmdrSheng": "çœ",
    "mmdrShiId": "å¸‚ID",   "mmdrShi": "å¸‚",
    "mmdrXianId": "å¿ID",  "mmdrXian": "å¿",
    "mmdrZhenId": "é•‡ID",  "mmdrZhen": "é•‡",
    "mmdrCunId": "æ‘ID",   "mmdrCun": "æ‘",

    "mmdrDeathJg": "ç±è´¯",
    "mmdrBirthYear": "å‡ºç”Ÿå¹´ä»½", "mmdrBirthMonth": "å‡ºç”Ÿæœˆä»½", "mmdrBirthDay": "å‡ºç”Ÿæ—¥æœŸ",
    "mmdrRdYear": "å…¥å…šå¹´ä»½",   "mmdrRdMonth": "å…¥å…šæœˆä»½",   "mmdrRdDay": "å…¥å…šæ—¥æœŸ",
    "mmdrWorkYear": "å‚å·¥å¹´ä»½", "mmdrWorkMonth": "å‚å·¥æœˆä»½", "mmdrWorkDay": "å‚å·¥æ—¥æœŸ",

    "mmdrZzmmId": "æ”¿æ²»é¢è²ŒID", "mmdrZzmm": "æ”¿æ²»é¢è²Œ",
    "mmdrUnit": "æ‰€åœ¨å•ä½", "mmdrJob": "èŒåŠ¡",

    "mmdrDeathYear": "ç‰ºç‰²å¹´ä»½", "mmdrDeathMonth": "ç‰ºç‰²æœˆä»½", "mmdrDeathDay": "ç‰ºç‰²æ—¥æœŸ",
    "mmdrDeathPlace": "ç‰ºç‰²åœ°ç‚¹",

    "mmdrBuryCode": "å®‰è‘¬åœ°ä»£ç ", "mmdrBury": "å®‰è‘¬åœ°",
    "mmdrCemeteryId": "é™µå›­ID",  "mmdrCemetery": "é™µå›­åç§°",
    "mmdrBuryPlace": "å®‰è‘¬åœ°å€",

    "mmdrDeeds": "ä¸»è¦äº‹è¿¹", "mmdrDeathCause": "ç‰ºç‰²åŸå› ", "mmdrHonor": "è£èª‰ç§°å·",

    "mmdrSbdwShengId": "ç”³æŠ¥çœID",  "mmdrSbdwSheng": "ç”³æŠ¥çœ",
    "mmdrSbdwShiId": "ç”³æŠ¥å¸‚ID",    "mmdrSbdwShi": "ç”³æŠ¥å¸‚",
    "mmdrSbdwXianId": "ç”³æŠ¥å¿ID",   "mmdrSbdwXian": "ç”³æŠ¥å¿",

    # å…¶ä½™è¾ƒå°‘ç”¨å­—æ®µå¯ç»§ç»­åœ¨è¿™é‡Œè¡¥å……â€¦â€¦
    "page_num": "æ¥æºé¡µç "
}

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 5. æ¥å£ç­¾å & HTTP
TOKEN_URL  = "https://yinglie.chinamartyrs.gov.cn/web-api/getToken"
SEARCH_URL = "https://yinglie.chinamartyrs.gov.cn/web-api/api/martyrs/search"

md5 = lambda s: hashlib.md5(s.encode()).hexdigest().upper()
def autograph(payload: Dict[str, Any], ts: int) -> str:
    kv = "&".join(f"{k}={payload[k]}" for k in sorted(payload) if payload[k])
    return md5(kv + f"&key=B28C665759654EF6A923F18888888888&timestamp={ts}")

def hdr(token: str | None = None,
        sign : str | None = None,
        ts   : int  | None = None,
        nosig: bool = False) -> Dict[str, str]:
    h = {"User-Agent": UA,
         "Origin": "https://www.chinamartyrs.gov.cn",
         "Referer": "https://www.chinamartyrs.gov.cn/",
         "Content-Type": "application/x-www-form-urlencoded; charset=UTF-8"}
    if token: h.update({"authorization": token, "user-token": token})
    if sign:  h["autograph"] = sign
    if ts:    h["timestamp"] = str(ts)
    if nosig: h["noSignature"] = "true"
    return h

# ---------- get_token ----------
def get_token(sess: requests.Session) -> str:
    for i in range(1, MAX_RETRY + 1):
        try:
            js = sess.post(TOKEN_URL, headers=hdr(nosig=True),
                           timeout=TIMEOUT).json()
            if js.get("code") == 200:
                return js["data"]["token"]
            log.warning("get_token è¿”å›å¼‚å¸¸ï¼š%s", js)
        except Exception as e:
            log.warning("get_token ç¬¬ %d/%d æ¬¡å¤±è´¥ï¼š%s", i, MAX_RETRY, e)
        time.sleep(min(2 ** (i - 1), args.cool_time))
    raise RuntimeError("get_token è¿ç»­å¤±è´¥")

# ---------- fetch_page ----------
def fetch_page(sess: requests.Session, token: str, page: int) -> Dict[str, Any]:
    payload = {"mmdrName": "", "pageNum": str(page), "pageSize": str(PAGE_SIZE),
               "Params": json.dumps({"beginTime": "", "endTime": ""}, ensure_ascii=False),
               "mmdrShengId": ""}
    ts   = int(time.time() * 1000)
    sign = autograph(payload, ts)

    for i in range(1, MAX_RETRY + 1):
        try:
            js = sess.post(SEARCH_URL, headers=hdr(token, sign, ts),
                           data=payload, timeout=TIMEOUT).json()
            if js.get("code") == 200:
                return js
            log.warning("page %d è¿”å›å¼‚å¸¸ï¼š%s", page, js)
        except Exception as e:
            log.warning("page %d ç¬¬ %d/%d æ¬¡å¤±è´¥ï¼š%s", page, i, MAX_RETRY, e)
        time.sleep(min(2 ** (i - 1), args.cool_time))
    raise RuntimeError(f"page {page} è¿ç»­å¤±è´¥")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 6. æ–­ç‚¹ / å»é‡ / åˆ†ç‰‡
def load_ids() -> Set[str]:
    return pickle.load(open(ID_CACHE, "rb")) if os.path.exists(ID_CACHE) else set()

def save_ids(ids: Set[str]) -> None:
    pickle.dump(ids, open(ID_CACHE, "wb"))

def read_page() -> int:
    try:
        return max(1, int(open(PAGE_FILE, encoding="utf-8").read().strip()))
    except Exception:
        return 1

def write_page(next_page: int) -> None:
    open(PAGE_FILE, "w", encoding="utf-8").write(str(next_page))

def cpath(idx: int) -> str:
    return os.path.join(TMP_DIR, f"chunk_{idx:05d}.csv")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 7. ä¸»æµç¨‹
def crawl() -> None:
    seen         = load_ids()
    chunk_rows   = []

    start_page   = read_page()
    total_pages  = min(args.pages, TOTAL_PAGES) if args.pages else TOTAL_PAGES
    cidx         = (start_page - 1) // CHUNK_PAGES

    with requests.Session() as sess:
        token = get_token(sess)
        log.info("token ok Â· ä»ç¬¬ %d é¡µå¼€å§‹ï¼Œå…±éœ€æŠ“ %d é¡µ", start_page, total_pages)

        bar = tqdm(total=total_pages, initial=start_page - 1,
                   unit="é¡µ", ncols=90, colour="cyan")

        for page in range(start_page, total_pages + 1):

            js = fetch_page(sess, token, page)
            for r in js["rows"]:
                gid = r["mmdrGuid"]
                if gid in seen:
                    continue
                seen.add(gid)

                for k in ("mmdrSheng", "mmdrShi", "mmdrXian"):
                    if not r.get(k) and r.get(k + "Id"):
                        r[k] = addr(r[k + "Id"])
                r["ID"]       = gid
                r["page_num"] = page
                chunk_rows.append(r)

            # é™é€Ÿ
            if page % args.cool_every == 0:
                time.sleep(args.cool_time)
            else:
                time.sleep(args.sleep_base + random.random() * args.sleep_jitter)

            # è¿›åº¦ & æ–­ç‚¹
            bar.update(1)
            write_page(page + 1)

            # åˆ†ç‰‡
            if page % CHUNK_PAGES == 0 or page == total_pages:
                if chunk_rows:
                    df  = pd.DataFrame(chunk_rows)
                    df  = df.rename(columns=column_explanations, errors="ignore")
                    df.to_csv(cpath(cidx), index=False, encoding="utf-8-sig")
                    log.info("ğŸ“¦ CSV åˆ†ç‰‡ %-18s | %d è¡Œ", f"chunk_{cidx:05d}.csv", len(df))
                chunk_rows.clear()
                cidx += 1
                save_ids(seen)

        bar.close()
        log.info("âœ¨ å…¨éƒ¨å®Œæˆï¼æŠ“å– %d é¡µï¼Œå»é‡å %d è®°å½•", total_pages, len(seen))
        write_page(1)

if __name__ == "__main__":
    try:
        crawl()
    except KeyboardInterrupt:
        log.warning("â¹ï¸ æ‰‹åŠ¨ä¸­æ–­ï¼Œä¿å­˜è¿›åº¦â€¦")
        save_ids(load_ids())
        sys.exit(0)
